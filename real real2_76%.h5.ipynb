{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2760dd30",
   "metadata": {
    "papermill": {
     "duration": 0.008479,
     "end_time": "2024-09-12T19:17:46.221881",
     "exception": false,
     "start_time": "2024-09-12T19:17:46.213402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f429d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5886d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Set GPU configuration to avoid memory issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe162caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = (299, 299)  # Target image size for InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc46015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:18:04.793054Z",
     "iopub.status.busy": "2024-09-12T19:18:04.792623Z",
     "iopub.status.idle": "2024-09-12T19:18:18.343040Z",
     "shell.execute_reply": "2024-09-12T19:18:18.342049Z"
    },
    "papermill": {
     "duration": 13.565442,
     "end_time": "2024-09-12T19:18:18.345757",
     "exception": false,
     "start_time": "2024-09-12T19:18:04.780315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory, label):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory containing images.\n",
    "    - label (int): Label for the samples (0 for real, 1 for fake).\n",
    "    \n",
    "    Returns:\n",
    "    - data (np.array): Array of processed images.\n",
    "    - labels (np.array): Array of corresponding labels.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.jpeg', '.jpg', '.png')):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, IMG_SIZE)\n",
    "                img = img_to_array(img)\n",
    "                img = preprocess_input(img)\n",
    "                data.append(img)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b08c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:18:18.369524Z",
     "iopub.status.busy": "2024-09-12T19:18:18.368872Z",
     "iopub.status.idle": "2024-09-12T19:18:32.977591Z",
     "shell.execute_reply": "2024-09-12T19:18:32.976464Z"
    },
    "papermill": {
     "duration": 14.62386,
     "end_time": "2024-09-12T19:18:32.980643",
     "exception": false,
     "start_time": "2024-09-12T19:18:18.356783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load real and fake images\n",
    "REAL_DIR = \"./DFFD/real\"\n",
    "FAKE_DIR = \"./DFFD/fake\"\n",
    "\n",
    "x_real, y_real = load_images_from_directory(REAL_DIR, label=0)  # Label 0 for real\n",
    "x_fake, y_fake = load_images_from_directory(FAKE_DIR, label=1)  # Label 1 for fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb72ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:18:33.004417Z",
     "iopub.status.busy": "2024-09-12T19:18:33.004009Z",
     "iopub.status.idle": "2024-09-12T19:18:34.191706Z",
     "shell.execute_reply": "2024-09-12T19:18:34.190745Z"
    },
    "papermill": {
     "duration": 1.202464,
     "end_time": "2024-09-12T19:18:34.194186",
     "exception": false,
     "start_time": "2024-09-12T19:18:32.991722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine real and fake data\n",
    "x_data = np.concatenate([x_real, x_fake], axis=0)\n",
    "y_data = np.concatenate([y_real, y_fake], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791ec00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a04a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea4b109",
   "metadata": {
    "papermill": {
     "duration": 0.039394,
     "end_time": "2024-09-12T19:33:32.858757",
     "exception": false,
     "start_time": "2024-09-12T19:33:32.819363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the feature extractor using InceptionV3\n",
    "def create_feature_extractor():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    feature_extractor = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def create_model():\n",
    "    feature_extractor = create_feature_extractor()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(299, 299, 3)))  # Input shape for single images\n",
    "    model.add(feature_extractor)  # Feature extraction\n",
    "    model.add(Dense(64, activation='relu'))  # Dense layer for intermediate features\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer: binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e57d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 2048)              21802784  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                131136    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,933,985\n",
      "Trainable params: 21,899,553\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and summarize the model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4388b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  patience=3, \n",
    "                                  verbose=1, \n",
    "                                  factor=0.5, \n",
    "                                  min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edb91887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 56s 451ms/step - loss: 0.5531 - accuracy: 0.7494 - val_loss: 15.0733 - val_accuracy: 0.3341 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 41s 384ms/step - loss: 0.5388 - accuracy: 0.7488 - val_loss: 620.3170 - val_accuracy: 0.3598 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 42s 388ms/step - loss: 0.5279 - accuracy: 0.7717 - val_loss: 3.7357 - val_accuracy: 0.6449 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 0.5205 - accuracy: 0.7740 - val_loss: 0.9973 - val_accuracy: 0.6192 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.5688 - accuracy: 0.7196 - val_loss: 4398.3774 - val_accuracy: 0.3341 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 43s 398ms/step - loss: 0.5774 - accuracy: 0.7149 - val_loss: 30.7413 - val_accuracy: 0.4579 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 43s 398ms/step - loss: 0.5529 - accuracy: 0.7418 - val_loss: 0.6494 - val_accuracy: 0.7079 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 60s 558ms/step - loss: 0.5268 - accuracy: 0.7512 - val_loss: 1.8885 - val_accuracy: 0.7383 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 261s 2s/step - loss: 0.5357 - accuracy: 0.7629 - val_loss: 4.3078 - val_accuracy: 0.6145 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.7641\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "107/107 [==============================] - 226s 2s/step - loss: 0.5243 - accuracy: 0.7641 - val_loss: 0.7987 - val_accuracy: 0.6869 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b046a5760>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=16), \n",
    "          epochs=10, \n",
    "          validation_data=(x_val, y_val), \n",
    "          callbacks=[lr_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16d000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('real2_76%.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37dcd60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 764ms/step\n",
      "The image is classified as REAL.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('real2_76%.h5')\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (299, 299)  # Target image size for InceptionV3\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "    - img (np.array): Preprocessed image array.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found or cannot be loaded.\")\n",
    "    \n",
    "    # Resize and preprocess the image\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)  # Preprocess using InceptionV3 preprocessing\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def predict_image_class(image_path):\n",
    "    \"\"\"\n",
    "    Predict the class of an image using the loaded model.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "    - class_label (int): Predicted class label (0 for real, 1 for fake).\n",
    "    \"\"\"\n",
    "    img = preprocess_image(image_path)\n",
    "    prediction = model.predict(img)\n",
    "    class_label = (prediction > 0.5).astype(int)  # Convert prediction to class label\n",
    "    return class_label[0][0]\n",
    "\n",
    "# Example usage\n",
    "image_path = 'DFFD/real/real_2_198.png'  # Replace with your image path\n",
    "predicted_class = predict_image_class(image_path)\n",
    "\n",
    "if predicted_class == 0:\n",
    "    print(\"The image is classified as REAL.\")\n",
    "else:\n",
    "    print(\"The image is classified as FAKE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181ba70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 858837,
     "sourceId": 16880,
     "sourceType": "competition"
    },
    {
     "datasetId": 5682694,
     "sourceId": 9370003,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 954.318661,
   "end_time": "2024-09-12T19:33:36.023069",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-12T19:17:41.704408",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
